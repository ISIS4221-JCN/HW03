\section{Introducción}

Dentro del área de procesamiento de lenguaje natural (NLP) uno de los tópicos de más relevancia es sin duda la representación de palabras. Cualquier modelo matemático que se desee formular a partir de palabras, ya sea un modelo de lenguaje, o para resolver tareas de clasificación, reconocimiento de entidades nombradas, etc. requiere de una entrada numérica. En principio uno podría pensar en simplemente asignar un número a cada palabra del vocabulario. Sin embargo, las palabras como tal no tienen una organización ordinal (no tiene sentido que una palabra sea dos veces más que la otra). \\

Una solución sencilla a esto es utilizar un \textit{One-Hot Encoder}, una transformación que codifica la palabra en un vector del tamaño del vocabulario. Este se construye con ceros en todo el vector salvo en la posición asignada a la palabra. Esto resuelve el problema de la falta de orden de la palabras, pues ahora todas se encuentran a una misma distancia vectorial. No obstante, esto presenta nuevos retos, como la creciente dimensionalidad de la entrada (que ahora es un vector del tamaño del vocabulario) y lo disperso que llega a ser este vector (prácticamente lleno de ceros en casi todas sus posiciones). \\

Ahora bien, una solución bastante ingeniosa para esto son los \textit{embeddings}. De forma general, estos son solo otra forma de representar las palabras en un espacio vectorial. No obstante, a diferencia de los \textit{One-Hot Encoders}, con los \textit{embeddings} se busca cuantificar la semántica de las palabras. Esto hace posible que ahora existan relaciones e incluso operaciones que tienen sentido tanto a nivel semántico como cuantitativo. Con esta representación dos palabras semánticamente similares pueden estar ahora más cerca la una de la otra dentro del espacio vectorial. \\

Así las cosas, en este taller se ponen a prueba dos propuestas de \textit{embeddings} para una tarea de clasificación. Por un lado, se construyen los \textit{embeddings} con el modelo de word2vec, presentado por google en 2013 y por el otro se se entrenan los \textit{embeddings} directamente dentro de la arquitectura de una red neuronal profunda. Estos embeddings se utilizan para entrenar un modelo de clasifcación que busca detectar que personaje dice una frase en las populares series de \textit{Friends} y \textit{Los Simpsons}.

\newpage