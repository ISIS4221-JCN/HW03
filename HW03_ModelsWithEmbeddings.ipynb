{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW03: ML Models w/ Embedding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports CSV to open raw data\n",
    "import csv\n",
    "\n",
    "# Imports numpy to read label data\n",
    "import numpy as np\n",
    "\n",
    "# Sets autocompletion\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix):\n",
    "    \"\"\" Loads the data from raw (preprocessed) datasets.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): string with path prefix to datasets.\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionary with the splitted dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalizes the output dictionary\n",
    "    output_dict = {\n",
    "        \"train\":None,\n",
    "        \"test\":None,\n",
    "        \"validation\":None\n",
    "    }\n",
    "    \n",
    "    # Intializes the suffix dictionary\n",
    "    suffix_dict = {\n",
    "        \"train\": [\"X_train.csv\", \"y_train.csv\"],\n",
    "        \"test\": [\"X_test.csv\", \"y_test.csv\"],\n",
    "        \"validation\": [\"X_val.csv\", \"y_val.csv\"],\n",
    "    }\n",
    "    \n",
    "    # Iterates over \n",
    "    for dataset in output_dict.keys():\n",
    "        \n",
    "        # Retrieves suffixes from dictionaty\n",
    "        x_suffix, y_suffix = suffix_dict[dataset]\n",
    "        \n",
    "        # Initializes list for data\n",
    "        x_data = []\n",
    "        \n",
    "        # Loads X data into a list\n",
    "        with open(prefix + x_suffix, 'r', encoding='latin-1') as data:\n",
    "            for row in csv.reader(data):\n",
    "                \n",
    "                # Creates a base string\n",
    "                sentence_string = \"\"\n",
    "                \n",
    "                # Iterates over subwords and appends to string\n",
    "                for subword in row:\n",
    "                    sentence_string += \" \"\n",
    "                    sentence_string += subword\n",
    "                \n",
    "                # Appends the sentence string\n",
    "                x_data.append([sentence_string])\n",
    "        \n",
    "        # Converts input data to numpy array\n",
    "        x_data = np.asarray(x_data)\n",
    "        \n",
    "        # Loads labels\n",
    "        y_data = np.loadtxt(prefix + y_suffix)\n",
    "               \n",
    "        # Stores data into dictionary\n",
    "        output_dict[dataset] = [x_data, y_data]\n",
    "\n",
    "    # Returns output file\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dataset_files(data, path):\n",
    "    \"\"\" Stores the data dictionaries into CSV files.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): data that will be converted.\n",
    "        label_dict (dict): labels dictionary.\n",
    "        path (str): prefix to dataset folder.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Creates the vectors for the features\n",
    "    x_train = np.squeeze(data[\"train\"][0])\n",
    "    x_val = np.squeeze(data[\"validation\"][0])\n",
    "    x_test = np.squeeze(data[\"test\"][0])\n",
    "    \n",
    "    # Defines the function to create the label vector\n",
    "    def one_hot_to_index(data):\n",
    "        \n",
    "        # Creates the output vector\n",
    "        output = []\n",
    "        \n",
    "        # Iterates and store the class index\n",
    "        for label in data:\n",
    "            for i, val in enumerate(label):\n",
    "                if val == 1:\n",
    "                    output.append(i)\n",
    "                    \n",
    "        # Converts to numpy array and returns the vector\n",
    "        return np.asarray(output)\n",
    "    \n",
    "    # Creates a vector with the class index\n",
    "    y_train = one_hot_to_index(data[\"train\"][1])\n",
    "    y_val = one_hot_to_index(data[\"validation\"][1])\n",
    "    y_test = one_hot_to_index(data[\"test\"][1])\n",
    "    \n",
    "    # Merges the features and data\n",
    "    train_data = np.transpose(np.vstack((x_train, y_train)))\n",
    "    val_data = np.transpose(np.vstack((x_val, y_val)))\n",
    "    test_data = np.transpose(np.vstack((x_test, y_test)))\n",
    "    \n",
    "    # Stores the datasets\n",
    "    np.savetxt(path + \"train_ds.csv\", train_data,\n",
    "        delimiter=',', fmt=\"%s,%s\", header=\"sentence, character\",\n",
    "        comments=\"\"\n",
    "    )\n",
    "    \n",
    "    np.savetxt(path + \"val_ds.csv\", val_data,\n",
    "        delimiter=',', fmt=\"%s,%s\", header=\"sentence, character\",\n",
    "        comments=\"\"\n",
    "    )\n",
    "    \n",
    "    np.savetxt(path + \"test_ds.csv\", test_data,\n",
    "        delimiter=',', fmt=\"%s,%s\", header=\"sentence, character\",\n",
    "        comments=\"\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data as preprocessed\n",
    "data = load_data(\"./data/simpsons/\")\n",
    "\n",
    "# Stores merged CSV files\n",
    "store_dataset_files(data, \"./data/simpsons/\")\n",
    "\n",
    "# Loads data as preprocessed\n",
    "data = load_data(\"./data/friends/\")\n",
    "\n",
    "# Stores merged CSV files\n",
    "store_dataset_files(data, \"./data/friends/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the best ML library ever: Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imports the Keras API for Tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Imports the layers, optimizers and metrics from Keras\n",
    "from tensorflow.keras import layers, optimizers, metrics\n",
    "\n",
    "# Imports the callbacks submodule from Keras API\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fcn_model(model_name, train_data, vocabulary_size, seq_len, embedding_dim, layers_def, output_dim):\n",
    "    \"\"\" Creates a fully connected deep model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): name of the model.\n",
    "        train_data (np.array): data to train the encoder layer.\n",
    "        vocabulary_size (int): maximum size for vocabulary.\n",
    "        seq_len (int): maximum size for sequence length.\n",
    "        embeedding_dim (int): embedding dimensionality.\n",
    "        layers_def (list): list with layers definition.\n",
    "        output_dim (int): output dimension for model.\n",
    "        \n",
    "    Returns:\n",
    "        (tf.keras.Sequential):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Creates the encoder layer\n",
    "    encoder = layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=vocabulary_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=seq_len\n",
    "    )\n",
    "\n",
    "    # Adapts the training dataset\n",
    "    encoder.adapt(train_data)\n",
    "    \n",
    "    # Creates the root model\n",
    "    model = keras.Sequential(name=model_name)\n",
    "    \n",
    "    # Adds an input layer\n",
    "    model.add(keras.Input(shape=(1,), dtype=tf.string))\n",
    "    \n",
    "    # Adds the encoder layer\n",
    "    model.add(encoder)\n",
    "    \n",
    "    # Adds the embedding layer\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=embedding_dim,\n",
    "    ))\n",
    "\n",
    "    # Flattens embedding data\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    \n",
    "    # Adds the hidden layers\n",
    "    for n_hidden in layers_def:\n",
    "        model.add(layers.Dense(n_hidden))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Adds the output layer\n",
    "    model.add(layers.Dense(output_dim, activation=\"softmax\"))\n",
    "    \n",
    "    # Returns the model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recursive_model(\n",
    "    model_name,\n",
    "    dataset,\n",
    "    vectorization,\n",
    "    vocabulary_size,\n",
    "    embedding_dim, \n",
    "    seq_len, \n",
    "    layers_def, \n",
    "    output_dim\n",
    "):\n",
    "    \n",
    "    # Creates the encoder layer\n",
    "    if vectorization == \"int\":\n",
    "        encoder = layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=vocabulary_size,\n",
    "            output_mode=vectorization,\n",
    "            output_sequence_length=seq_len\n",
    "        )\n",
    "        \n",
    "        # Adapts the training dataset\n",
    "        encoder.adapt(np.asarray(dataset[\"train\"][0], dtype=np.str))\n",
    "        \n",
    "    else:\n",
    "        encoder = layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=vocabulary_size,\n",
    "            output_mode=vectorization\n",
    "        )\n",
    "        \n",
    "        # Adapts the training dataset\n",
    "        encoder.adapt(np.asarray(dataset[\"train\"][0], dtype=np.str))\n",
    "    \n",
    "    # Creates the root model\n",
    "    model = keras.Sequential(name=model_name)\n",
    "    \n",
    "    # Adds an input layer\n",
    "    model.add(keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "    # Adds the encoder layer\n",
    "    model.add(encoder)\n",
    "    \n",
    "    # Adds the embedding layer\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=embedding_dim,\n",
    "    ))\n",
    "    \n",
    "    # Creates a flag to add a flatten layer\n",
    "    _added_flatten = False\n",
    "    \n",
    "    # Adds the hidden layers\n",
    "    for n_hidden in layers_def:\n",
    "        \n",
    "        # Adds a LSTM layer\n",
    "        if n_hidden[0] == \"LSTM\":\n",
    "            if not _added_flatten:\n",
    "                model.add(layers.LSTM(\n",
    "                    n_hidden[1], \n",
    "                    return_sequences=n_hidden[2]\n",
    "                ))\n",
    "            else:\n",
    "                raise Exception(\"Cannot add a recursive layer after a dense layer\")\n",
    "        \n",
    "        # Adds a bidirectional LSTM layer\n",
    "        elif n_hidden[0] == \"BI_LSTM\":\n",
    "            if not _added_flatten:\n",
    "                model.add(layers.Bidirectional(\n",
    "                    layers.LSTM(\n",
    "                        n_hidden[1],\n",
    "                        return_sequences=n_hidden[2]\n",
    "                )))\n",
    "            else:\n",
    "                raise Exception(\"Cannot add a recursive layer after a dense layer\")\n",
    "            \n",
    "        # Adds a RNN layer\n",
    "        elif n_hidden[0] == \"RNN\":\n",
    "            if not _added_flatten:\n",
    "                model.add(layers.SimpleRNN(\n",
    "                    n_hidden[1], \n",
    "                    return_sequences=n_hidden[2]\n",
    "                ))\n",
    "            else:\n",
    "                raise Exception(\"Cannot add a recursive layer after a dense layer\")\n",
    "                \n",
    "        # Adds a GRU layer\n",
    "        elif n_hidden[0] == \"GRU\":\n",
    "            if not _added_flatten:\n",
    "                model.add(layers.GRU(\n",
    "                    n_hidden[1], \n",
    "                    return_sequences=n_hidden[2]\n",
    "                ))\n",
    "            else:\n",
    "                raise Exception(\"Cannot add a recursive layer after a dense layer\")\n",
    "            \n",
    "        # Adds a dense model\n",
    "        else:\n",
    "            \n",
    "            # Adds a flatten layer if necessary\n",
    "            if not _added_flatten:\n",
    "                model.add(layers.Flatten())\n",
    "                _added_flatten = True\n",
    "            \n",
    "            # Adds the dense layer and a dropout layer\n",
    "            model.add(layers.Dense(n_hidden[1]))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Adds the output layer\n",
    "    model.add(layers.Dense(output_dim, activation=\"softmax\"))\n",
    "\n",
    "    # Returns the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolide_model(model,lr=1e-3):\n",
    "    \n",
    "    # Compiles the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(lr=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Returns the compiled model\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_directory():\n",
    "    \"\"\" Creates the directories to store the results. \"\"\"\n",
    "    \n",
    "    # Imports the OS library\n",
    "    import os\n",
    "    \n",
    "    # Checks that the results directory exists\n",
    "    if not os.path.isdir(\"./doc/data/em_models/\"):\n",
    "        os.mkdir(\"./doc/data/em_models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_file(name):\n",
    "    \n",
    "    # Imports required functions\n",
    "    from os import remove\n",
    "    from os.path import exists\n",
    "    \n",
    "    # Creates the full path for file\n",
    "    path = \"./doc/data/em_models/\" + name + \".csv\"\n",
    "    \n",
    "    # If file exists it is deleted\n",
    "    if exists(path):\n",
    "        remove(path)\n",
    "        \n",
    "    # Creates the file and writers the header row\n",
    "    with open(path, \"w+\") as file:\n",
    "        csv_file = csv.writer(file, delimiter=\",\")\n",
    "        csv_file.writerow([\n",
    "            \"name\",\n",
    "            \"model\",\n",
    "            'accuracy',\n",
    "            'precision',\n",
    "            'recall',\n",
    "            \"F1\"\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_on_file(name, values):\n",
    "    \n",
    "    # Defines the full path\n",
    "    path = \"./doc/data/em_models/\" + name + \".csv\"\n",
    "    \n",
    "    # Creates the results list \n",
    "    results = []\n",
    "    \n",
    "    # Modifies to stores floats with format\n",
    "    for value in values:\n",
    "        if type(value) == str:\n",
    "            results.append(value)\n",
    "        elif type(value) == float:\n",
    "            results.append(\"{:5.4f}\".format(value))\n",
    "    \n",
    "    # Creates the file and writes the results\n",
    "    with open(path, \"a\") as file:\n",
    "        \n",
    "        # Creates the writer object\n",
    "        _writer = csv.writer(file, delimiter=\",\")\n",
    "        \n",
    "        # Writes the values into file\n",
    "        _writer.writerow(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(precision, recall):\n",
    "    numer = 2.0 * float(precision) * float(recall)\n",
    "    denom = float(precision) + float(recall)\n",
    "    try:\n",
    "        return numer / denom\n",
    "    except ZeroDivisionError as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line prevents TF crashing when using convolutional networks\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"embeddings\"\n",
    "\n",
    "# Sets a pool for embedding dimensionalities\n",
    "embedding_list = [5, 10, 15, 20, 25, 50, 100, 150]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Sequence length from preprocessing analysis\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 19\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# Creates the results folder\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1337 - accuracy: 0.5162 - precision: 0.6672 - recall: 0.2631\n",
      "Training model: simpsons_10\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1337 - accuracy: 0.5143 - precision: 0.6568 - recall: 0.2858\n",
      "Training model: simpsons_15\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1248 - accuracy: 0.5179 - precision: 0.6717 - recall: 0.2830\n",
      "Training model: simpsons_20\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1273 - accuracy: 0.5148 - precision: 0.6537 - recall: 0.2988\n",
      "Training model: simpsons_25\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1155 - accuracy: 0.5196 - precision: 0.6658 - recall: 0.2924\n",
      "Training model: simpsons_50\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1144 - accuracy: 0.5176 - precision: 0.6694 - recall: 0.2890\n",
      "Training model: simpsons_100\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1144 - accuracy: 0.5213 - precision: 0.6595 - recall: 0.3045\n",
      "Training model: simpsons_150\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1137 - accuracy: 0.5221 - precision: 0.6407 - recall: 0.3308\n",
      "Training model: friends_5\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7312 - accuracy: 0.2536 - precision: 0.5301 - recall: 0.0063\n",
      "Training model: friends_10\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "218/218 [==============================] - 1s 2ms/step - loss: 1.7193 - accuracy: 0.2674 - precision: 0.6099 - recall: 0.0123\n",
      "Training model: friends_15\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7297 - accuracy: 0.2600 - precision: 0.5621 - recall: 0.0123\n",
      "Training model: friends_20\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "218/218 [==============================] - 1s 2ms/step - loss: 1.7128 - accuracy: 0.2741 - precision: 0.5921 - recall: 0.0194\n",
      "Training model: friends_25\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7063 - accuracy: 0.2795 - precision: 0.5926 - recall: 0.0321\n",
      "Training model: friends_50\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7103 - accuracy: 0.2780 - precision: 0.5738 - recall: 0.0201\n",
      "Training model: friends_100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7012 - accuracy: 0.2824 - precision: 0.5708 - recall: 0.0353\n",
      "Training model: friends_150\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7020 - accuracy: 0.2869 - precision: 0.5702 - recall: 0.0396\n"
     ]
    }
   ],
   "source": [
    "# Iterates over dataset prefixes\n",
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Gets the prefix\n",
    "    prefix = prefix_dict[dataset]\n",
    "    \n",
    "    # Loads the data\n",
    "    data = load_data(prefix)\n",
    "    \n",
    "    # Gets the sequences length\n",
    "    seq_len = seq_len_dict[dataset]\n",
    "    \n",
    "    # Gets output dimension\n",
    "    output_dim = output_dict[dataset]\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    # Iterates over different embedding dimensionalities\n",
    "    for embedding in embedding_list:\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset + \"_\" + str(embedding)\n",
    "        \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Calculates the hidden layer size\n",
    "        hidden_layer = int((embedding + output_dim) / 2)\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_fcn_model(\n",
    "            model_name = name, \n",
    "            train_data = data[\"train\"][0],\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding,\n",
    "            layers_def=[hidden_layer],\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "                                       \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends index and model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Length dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"seq_len\"\n",
    "\n",
    "# Sets a pool for sequence lengths\n",
    "seq_len_list = [5, 10, 15, 20, 25, 50, 100]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_size_dict = {\n",
    "    \"simpsons\":150,\n",
    "    \"friends\":150\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1565 - accuracy: 0.4971 - precision: 0.6187 - recall: 0.2816\n",
      "Training model: simpsons_10\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1277 - accuracy: 0.5145 - precision: 0.6269 - recall: 0.3167\n",
      "Training model: simpsons_15\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1124 - accuracy: 0.5215 - precision: 0.6410 - recall: 0.3244\n",
      "Training model: simpsons_20\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1147 - accuracy: 0.5211 - precision: 0.6454 - recall: 0.3140\n",
      "Training model: simpsons_25\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1146 - accuracy: 0.5231 - precision: 0.6532 - recall: 0.3096\n",
      "Training model: simpsons_50\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1187 - accuracy: 0.5172 - precision: 0.6734 - recall: 0.2910\n",
      "Training model: simpsons_100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1194 - accuracy: 0.5203 - precision: 0.6296 - recall: 0.3410\n",
      "Training model: friends_5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7323 - accuracy: 0.2526 - precision: 0.5062 - recall: 0.0176\n",
      "Training model: friends_10\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7146 - accuracy: 0.2719 - precision: 0.5144 - recall: 0.0205\n",
      "Training model: friends_15\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7052 - accuracy: 0.2877 - precision: 0.5556 - recall: 0.0351\n",
      "Training model: friends_20\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6989 - accuracy: 0.2890 - precision: 0.5613 - recall: 0.0387\n",
      "Training model: friends_25\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6890 - accuracy: 0.2968 - precision: 0.5714 - recall: 0.0436\n",
      "Training model: friends_50\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6819 - accuracy: 0.2965 - precision: 0.6013 - recall: 0.0404\n",
      "Training model: friends_100\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.6853 - accuracy: 0.2988 - precision: 0.6026 - recall: 0.0459\n"
     ]
    }
   ],
   "source": [
    "# Iterates over dataset prefixes\n",
    "for _dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + _dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + _dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + _dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    # Iterates over sequence length list\n",
    "    for seq_len in seq_len_list:\n",
    "    \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[_dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[_dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = _dataset + \"_\" + str(seq_len)\n",
    "        \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the corresponding embedding size\n",
    "        embedding_size = embedding_size_dict[_dataset]\n",
    "        \n",
    "        # Calculates the hidden layer size\n",
    "        hidden_layer = int((embedding_size + output_dim) / 2)\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_fcn_model(\n",
    "            model_name = name, \n",
    "            train_data = data[\"train\"][0],\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=[hidden_layer],\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"vocabulary_size\"\n",
    "\n",
    "# Sets a pool for sequence lengths\n",
    "vocabulary_size_list = [1000, 2500, 5000, 10000, 15000]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_1000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1213 - accuracy: 0.5119 - precision: 0.6619 - recall: 0.2714\n",
      "Training model: simpsons_2500\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1137 - accuracy: 0.5208 - precision: 0.6485 - recall: 0.3042\n",
      "Training model: simpsons_5000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1146 - accuracy: 0.5207 - precision: 0.6402 - recall: 0.3219\n",
      "Training model: simpsons_10000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1126 - accuracy: 0.5218 - precision: 0.6348 - recall: 0.3385\n",
      "Training model: simpsons_15000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1218 - accuracy: 0.5206 - precision: 0.6346 - recall: 0.3319\n",
      "Training model: friends_1000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7090 - accuracy: 0.2709 - precision: 0.6183 - recall: 0.0165\n",
      "Training model: friends_2500\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6995 - accuracy: 0.2770 - precision: 0.5800 - recall: 0.0359\n",
      "Training model: friends_5000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6997 - accuracy: 0.2887 - precision: 0.5421 - recall: 0.0462\n",
      "Training model: friends_10000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6974 - accuracy: 0.2854 - precision: 0.5723 - recall: 0.0392\n",
      "Training model: friends_15000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7014 - accuracy: 0.2810 - precision: 0.5421 - recall: 0.0508\n"
     ]
    }
   ],
   "source": [
    "# Iterates over dataset prefixes\n",
    "for _dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + _dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + _dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + _dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    # Iterates over sequence length list\n",
    "    for vocabulary_size in vocabulary_size_list:\n",
    "    \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[_dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[_dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = _dataset + \"_\" + str(vocabulary_size)\n",
    "        \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[_dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[_dataset]\n",
    "        \n",
    "        # Calculates the hidden layer size\n",
    "        hidden_layer = int((embedding_size + output_dim) / 2)\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_fcn_model(\n",
    "            model_name = name, \n",
    "            train_data = data[\"train\"][0],\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=[hidden_layer],\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"fcn_models\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [25],\n",
    "    [50],\n",
    "    [75],\n",
    "    [100],\n",
    "    [125],\n",
    "    [150],\n",
    "    [25, 25],\n",
    "    [50, 25],\n",
    "    [50, 50],\n",
    "    [75, 50],\n",
    "    [100, 50],\n",
    "    [150, 75],\n",
    "    [75, 50, 25],\n",
    "    [100, 50, 25],\n",
    "    [150, 75, 25],\n",
    "    [150, 150, 150]\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_25\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1105 - accuracy: 0.5244 - precision: 0.6416 - recall: 0.3286\n",
      "Training model: simpsons_50\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1162 - accuracy: 0.5208 - precision: 0.6671 - recall: 0.2903\n",
      "Training model: simpsons_75\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1141 - accuracy: 0.5225 - precision: 0.6415 - recall: 0.3254\n",
      "Training model: simpsons_100\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1158 - accuracy: 0.5206 - precision: 0.6489 - recall: 0.3112\n",
      "Training model: simpsons_125\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1148 - accuracy: 0.5228 - precision: 0.6251 - recall: 0.3482\n",
      "Training model: simpsons_150\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1177 - accuracy: 0.5222 - precision: 0.6173 - recall: 0.3596\n",
      "Training model: simpsons_25_25\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1206 - accuracy: 0.5187 - precision: 0.6296 - recall: 0.3314\n",
      "Training model: simpsons_50_25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1214 - accuracy: 0.5168 - precision: 0.6382 - recall: 0.3184\n",
      "Training model: simpsons_50_50\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1221 - accuracy: 0.5191 - precision: 0.6369 - recall: 0.3247\n",
      "Training model: simpsons_75_50\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1241 - accuracy: 0.5190 - precision: 0.6417 - recall: 0.3125\n",
      "Training model: simpsons_100_50\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1196 - accuracy: 0.5237 - precision: 0.6302 - recall: 0.3401\n",
      "Training model: simpsons_150_75\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1224 - accuracy: 0.5171 - precision: 0.6366 - recall: 0.3236\n",
      "Training model: simpsons_75_50_25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.1226 - accuracy: 0.5203 - precision: 0.6352 - recall: 0.3208\n",
      "Training model: simpsons_100_50_25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1214 - accuracy: 0.5160 - precision: 0.6562 - recall: 0.3022\n",
      "Training model: simpsons_150_75_25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1269 - accuracy: 0.5143 - precision: 0.6304 - recall: 0.3229\n",
      "Training model: simpsons_150_150_150\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1216 - accuracy: 0.5222 - precision: 0.6332 - recall: 0.3380\n",
      "Training model: friends_25\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6969 - accuracy: 0.2871 - precision: 0.5801 - recall: 0.0369\n",
      "Training model: friends_50\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6988 - accuracy: 0.2910 - precision: 0.6004 - recall: 0.0386\n",
      "Training model: friends_75\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7019 - accuracy: 0.2882 - precision: 0.5532 - recall: 0.0522\n",
      "Training model: friends_100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6967 - accuracy: 0.2861 - precision: 0.5809 - recall: 0.0392\n",
      "Training model: friends_125\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6957 - accuracy: 0.2917 - precision: 0.5699 - recall: 0.0468\n",
      "Training model: friends_150\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6984 - accuracy: 0.2927 - precision: 0.5429 - recall: 0.0491\n",
      "Training model: friends_25_25\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7111 - accuracy: 0.2764 - precision: 0.5501 - recall: 0.0354\n",
      "Training model: friends_50_25\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7056 - accuracy: 0.2817 - precision: 0.5808 - recall: 0.0382\n",
      "Training model: friends_50_50\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7110 - accuracy: 0.2838 - precision: 0.5421 - recall: 0.0250\n",
      "Training model: friends_75_50\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7099 - accuracy: 0.2867 - precision: 0.5303 - recall: 0.0590\n",
      "Training model: friends_100_50\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7070 - accuracy: 0.2896 - precision: 0.5265 - recall: 0.0485\n",
      "Training model: friends_150_75\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7098 - accuracy: 0.2838 - precision: 0.5286 - recall: 0.0518\n",
      "Training model: friends_75_50_25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7160 - accuracy: 0.2760 - precision: 0.5572 - recall: 0.0426\n",
      "Training model: friends_100_50_25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7226 - accuracy: 0.2770 - precision: 0.5679 - recall: 0.0330\n",
      "Training model: friends_150_75_25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7184 - accuracy: 0.2777 - precision: 0.5242 - recall: 0.0466\n",
      "Training model: friends_150_150_150\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7182 - accuracy: 0.2920 - precision: 0.5184 - recall: 0.0546\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer)\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_fcn_model(\n",
    "            model_name = name, \n",
    "            train_data = data[\"train\"][0],\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"simple_rnn\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [(\"RNN\", 150, False), (\"DENSE\", 75)],\n",
    "    [(\"RNN\", 300, False), (\"DENSE\", 75)],\n",
    "    [(\"RNN\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"RNN\", 150, True), (\"RNN\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"RNN\", 300, True), (\"RNN\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)]\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_RNN150_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1455 - accuracy: 0.4969 - precision: 0.6179 - recall: 0.2963\n",
      "Training model: simpsons_RNN300_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1492 - accuracy: 0.5029 - precision: 0.6070 - recall: 0.3325\n",
      "Training model: simpsons_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1545 - accuracy: 0.4900 - precision: 0.6867 - recall: 0.2190\n",
      "Training model: simpsons_RNN150_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 5ms/step - loss: 1.1564 - accuracy: 0.5019 - precision: 0.6035 - recall: 0.3276\n",
      "Training model: simpsons_RNN300_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 5ms/step - loss: 1.1599 - accuracy: 0.4900 - precision: 0.6357 - recall: 0.2712\n",
      "Training model: friends_RNN150_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7685 - accuracy: 0.2453 - precision: 0.4211 - recall: 0.0184\n",
      "Training model: friends_RNN300_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7728 - accuracy: 0.2484 - precision: 0.4128 - recall: 0.0176\n",
      "Training model: friends_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7575 - accuracy: 0.2311 - precision: 0.4000 - recall: 0.0014\n",
      "Training model: friends_RNN150_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7637 - accuracy: 0.2354 - precision: 0.6774 - recall: 0.0030\n",
      "Training model: friends_RNN300_RNN150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 6ms/step - loss: 1.7745 - accuracy: 0.2473 - precision: 0.4312 - recall: 0.0135\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer[0]) + str(layer[1])\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_recursive_model(\n",
    "            model_name = name, \n",
    "            dataset= data,\n",
    "            vectorization=\"int\",\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"lstm\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [(\"LSTM\", 150, False), (\"DENSE\", 75)],\n",
    "    [(\"LSTM\", 300, False), (\"DENSE\", 75)],\n",
    "    [(\"LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"LSTM\", 150, True), (\"LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"LSTM\", 300, True), (\"LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)]\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_LSTM150_DENSE75\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1492 - accuracy: 0.5044 - precision: 0.6335 - recall: 0.3106\n",
      "Training model: simpsons_LSTM300_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1296 - accuracy: 0.5091 - precision: 0.6326 - recall: 0.3183\n",
      "Training model: simpsons_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 1.1348 - accuracy: 0.5137 - precision: 0.6316 - recall: 0.3160\n",
      "Training model: simpsons_LSTM150_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1778 - accuracy: 0.4777 - precision: 0.6064 - recall: 0.2771\n",
      "Training model: simpsons_LSTM300_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1403 - accuracy: 0.5003 - precision: 0.6480 - recall: 0.2900\n",
      "Training model: friends_LSTM150_DENSE75\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7393 - accuracy: 0.2750 - precision: 0.4491 - recall: 0.0449\n",
      "Training model: friends_LSTM300_DENSE75\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7422 - accuracy: 0.2734 - precision: 0.4638 - recall: 0.0561\n",
      "Training model: friends_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.7295 - accuracy: 0.2605 - precision: 0.5787 - recall: 0.0164\n",
      "Training model: friends_LSTM150_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7312 - accuracy: 0.2609 - precision: 0.5488 - recall: 0.0169\n",
      "Training model: friends_LSTM300_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7236 - accuracy: 0.2784 - precision: 0.5222 - recall: 0.0456\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer[0]) + str(layer[1])\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_recursive_model(\n",
    "            model_name = name, \n",
    "            dataset= data,\n",
    "            vectorization=\"int\",\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"bi_lstm\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [(\"BI_LSTM\", 150, False), (\"DENSE\", 75)],\n",
    "    [(\"BI_LSTM\", 300, False), (\"DENSE\", 75)],\n",
    "    [(\"BI_LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"BI_LSTM\", 150, True), (\"BI_LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"BI_LSTM\", 300, True), (\"BI_LSTM\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)]\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_BI_LSTM150_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1170 - accuracy: 0.5244 - precision: 0.6286 - recall: 0.3424\n",
      "Training model: simpsons_BI_LSTM300_DENSE75\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1131 - accuracy: 0.5186 - precision: 0.6613 - recall: 0.2944\n",
      "Training model: simpsons_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1337 - accuracy: 0.5008 - precision: 0.6516 - recall: 0.2734\n",
      "Training model: simpsons_BI_LSTM150_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 5ms/step - loss: 1.1294 - accuracy: 0.5094 - precision: 0.6577 - recall: 0.2904\n",
      "Training model: simpsons_BI_LSTM300_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 6ms/step - loss: 1.1375 - accuracy: 0.5120 - precision: 0.6337 - recall: 0.3214\n",
      "Training model: friends_BI_LSTM150_DENSE75\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 6ms/step - loss: 1.7106 - accuracy: 0.2870 - precision: 0.5016 - recall: 0.0450\n",
      "Training model: friends_BI_LSTM300_DENSE75\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 11ms/step - loss: 1.7227 - accuracy: 0.2833 - precision: 0.4809 - recall: 0.0577\n",
      "Training model: friends_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 7ms/step - loss: 1.7101 - accuracy: 0.2760 - precision: 0.5294 - recall: 0.0336\n",
      "Training model: friends_BI_LSTM150_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 10ms/step - loss: 1.7376 - accuracy: 0.2616 - precision: 0.4641 - recall: 0.0324\n",
      "Training model: friends_BI_LSTM300_BI_LSTM150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 4s 20ms/step - loss: 1.7232 - accuracy: 0.2635 - precision: 0.4914 - recall: 0.0245\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer[0]) + str(layer[1])\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_recursive_model(\n",
    "            model_name = name, \n",
    "            dataset= data,\n",
    "            vectorization=\"int\",\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"gru\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [(\"GRU\", 150, False), (\"DENSE\", 75)],\n",
    "    [(\"GRU\", 300, False), (\"DENSE\", 75)],\n",
    "    [(\"GRU\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"GRU\", 150, True), (\"GRU\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)],\n",
    "    [(\"GRU\", 300, True), (\"GRU\", 150, False), (\"DENSE\", 150), (\"DENSE\", 75), (\"DENSE\", 25)]\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_GRU150_DENSE75\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1482 - accuracy: 0.5119 - precision: 0.6253 - recall: 0.3200\n",
      "Training model: simpsons_GRU300_DENSE75\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 5ms/step - loss: 1.1627 - accuracy: 0.5035 - precision: 0.6089 - recall: 0.3229\n",
      "Training model: simpsons_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 4ms/step - loss: 1.1660 - accuracy: 0.5038 - precision: 0.6343 - recall: 0.2901\n",
      "Training model: simpsons_GRU150_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 5ms/step - loss: 1.1502 - accuracy: 0.5065 - precision: 0.6117 - recall: 0.3224\n",
      "Training model: simpsons_GRU300_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 7ms/step - loss: 1.1374 - accuracy: 0.5120 - precision: 0.6428 - recall: 0.2981\n",
      "Training model: friends_GRU150_DENSE75\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7442 - accuracy: 0.2431 - precision: 0.5571 - recall: 0.0175\n",
      "Training model: friends_GRU300_DENSE75\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7598 - accuracy: 0.2595 - precision: 0.4749 - recall: 0.0420\n",
      "Training model: friends_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 7ms/step - loss: 1.7599 - accuracy: 0.2344 - precision: 0.5380 - recall: 0.0122\n",
      "Training model: friends_GRU150_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 6ms/step - loss: 1.7905 - accuracy: 0.1738 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Training model: friends_GRU300_GRU150_DENSE150_DENSE75_DENSE25\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 7ms/step - loss: 1.7707 - accuracy: 0.2217 - precision: 0.5506 - recall: 0.0070\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer[0]) + str(layer[1])\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_recursive_model(\n",
    "            model_name = name, \n",
    "            dataset= data,\n",
    "            vectorization=\"int\",\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Search on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets task name\n",
    "task_name = \"bi_lstm_improved\"\n",
    "\n",
    "# Sets a pool for model definitions\n",
    "model_list = [\n",
    "    [(\"BI_LSTM\", 25, False), (\"DENSE\", 15)],\n",
    "    [(\"BI_LSTM\", 50, False), (\"DENSE\", 27)],\n",
    "    [(\"BI_LSTM\", 75, False), (\"DENSE\", 40)],\n",
    "    [(\"BI_LSTM\", 150, False), (\"DENSE\", 77)],\n",
    "    [(\"BI_LSTM\", 300, False), (\"DENSE\", 152)],\n",
    "]\n",
    "\n",
    "# Prefixes for dataset location\n",
    "prefix_dict  = {\n",
    "    \"simpsons\": \"./data/simpsons/\",\n",
    "    \"friends\": \"./data/friends/\"\n",
    "}\n",
    "\n",
    "# Output dimension dictionary\n",
    "output_dict = {\n",
    "    \"simpsons\": 4,\n",
    "    \"friends\": 6\n",
    "}\n",
    "\n",
    "# Sets a vocabulary size variable\n",
    "seq_len_dict = {\n",
    "    \"simpsons\": 15,\n",
    "    \"friends\": 20\n",
    "}\n",
    "\n",
    "# Sets the embedding size\n",
    "embedding_dict = {\n",
    "    \"simpsons\": 150,\n",
    "    \"friends\": 150\n",
    "}\n",
    "\n",
    "vocabulary_size_dict = {\n",
    "    \"simpsons\": 10000,\n",
    "    \"friends\": 15000\n",
    "}\n",
    "\n",
    "# Creates the folders\n",
    "create_results_directory()\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: simpsons_BI_LSTM25_DENSE15\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 6ms/step - loss: 1.1203 - accuracy: 0.5210 - precision: 0.6304 - recall: 0.3253\n",
      "Training model: simpsons_BI_LSTM50_DENSE27\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 5ms/step - loss: 1.1187 - accuracy: 0.5147 - precision: 0.6557 - recall: 0.3054\n",
      "Training model: simpsons_BI_LSTM75_DENSE40\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 5ms/step - loss: 1.1271 - accuracy: 0.5247 - precision: 0.6280 - recall: 0.3325\n",
      "Training model: simpsons_BI_LSTM150_DENSE77\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 6ms/step - loss: 1.1226 - accuracy: 0.5091 - precision: 0.6591 - recall: 0.2788\n",
      "Training model: simpsons_BI_LSTM300_DENSE152\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 3s 10ms/step - loss: 1.1275 - accuracy: 0.5213 - precision: 0.6099 - recall: 0.3817\n",
      "Training model: friends_BI_LSTM25_DENSE15\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.7303 - accuracy: 0.2678 - precision: 0.5351 - recall: 0.0229\n",
      "Training model: friends_BI_LSTM50_DENSE27\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7228 - accuracy: 0.2704 - precision: 0.5105 - recall: 0.0313\n",
      "Training model: friends_BI_LSTM75_DENSE40\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 5ms/step - loss: 1.7152 - accuracy: 0.2851 - precision: 0.4891 - recall: 0.0516\n",
      "Training model: friends_BI_LSTM150_DENSE77\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 7ms/step - loss: 1.7116 - accuracy: 0.2826 - precision: 0.5237 - recall: 0.0524\n",
      "Training model: friends_BI_LSTM300_DENSE152\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 11ms/step - loss: 1.7276 - accuracy: 0.2791 - precision: 0.4859 - recall: 0.0618\n"
     ]
    }
   ],
   "source": [
    "for dataset in prefix_dict.keys():\n",
    "    \n",
    "    # Defines the results filenames\n",
    "    train_name = task_name + \"_\" + dataset + \"_train\"\n",
    "    val_name = task_name + \"_\" + dataset + \"_val\"\n",
    "    test_name = task_name + \"_\" + dataset + \"_test\"\n",
    "    \n",
    "    # Creates the results file\n",
    "    create_results_file(train_name)\n",
    "    create_results_file(val_name)\n",
    "    create_results_file(test_name)\n",
    "    \n",
    "    # Initializes an index for models\n",
    "    model_index = 1\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        # Gets the prefix\n",
    "        prefix = prefix_dict[dataset]\n",
    "    \n",
    "        # Loads the data\n",
    "        data = load_data(prefix)\n",
    "    \n",
    "        # Gets output dimension\n",
    "        output_dim = output_dict[dataset]\n",
    "        \n",
    "        # Creates a name for the model\n",
    "        name = dataset\n",
    "        \n",
    "        # Adds layers definition to model name\n",
    "        for layer in model:\n",
    "            name += \"_\" + str(layer[0]) + str(layer[1])\n",
    "            \n",
    "        # Prints information\n",
    "        print(\"Training model: \" + name)\n",
    "        \n",
    "        # Sets the vocabulary size\n",
    "        vocabulary_size = vocabulary_size_dict[dataset]\n",
    "        \n",
    "        # Sets the sequence length for model\n",
    "        seq_len = seq_len_dict[dataset]\n",
    "        \n",
    "        # Sets the embedding size\n",
    "        embedding_size = embedding_dict[dataset]\n",
    "        \n",
    "        # Creates the model\n",
    "        model = create_recursive_model(\n",
    "            model_name = name, \n",
    "            dataset= data,\n",
    "            vectorization=\"int\",\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=embedding_size,\n",
    "            layers_def=model,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        \n",
    "        # Consolides the model\n",
    "        model = consolide_model(model, lr=1e-4)\n",
    "        \n",
    "        # Trains the model\n",
    "        history = model.fit(\n",
    "            x=data[\"train\"][0],\n",
    "            y=data[\"train\"][1],\n",
    "            batch_size=25,\n",
    "            epochs=50,\n",
    "            validation_data=(data[\"validation\"][0], data[\"validation\"][1]),\n",
    "            callbacks=[lr_decrease, early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Creates the training results\n",
    "        train_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"accuracy\"][-1],\n",
    "            history.history[\"precision\"][-1],\n",
    "            history.history[\"recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "        ]\n",
    "\n",
    "        # Stores the training results\n",
    "        write_results_on_file(train_name, train_results)\n",
    "        \n",
    "        # Creates the validation results\n",
    "        val_results = [\n",
    "            name,\n",
    "            str(model_index),\n",
    "            history.history[\"val_accuracy\"][-1],\n",
    "            history.history[\"val_precision\"][-1],\n",
    "            history.history[\"val_recall\"][-1],\n",
    "            calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "        ]\n",
    "        \n",
    "        # Stores the validation results\n",
    "        write_results_on_file(val_name, val_results)\n",
    "\n",
    "        # Evaluates the model\n",
    "        results = model.evaluate(\n",
    "            x = data[\"test\"][0],\n",
    "            y = data[\"test\"][1]\n",
    "        )\n",
    "        \n",
    "        # Appends model to list\n",
    "        results.insert(0, str(model_index))\n",
    "        results.insert(0, name)\n",
    "        \n",
    "        # Removes loss from results\n",
    "        results.pop(2)\n",
    "        \n",
    "        # Adds F1 score to test\n",
    "        results.append(calculate_f1_score(results[3], results[4]))\n",
    "        \n",
    "        # Writes results\n",
    "        write_results_on_file(test_name, results)\n",
    "        \n",
    "        # Increments the model index\n",
    "        model_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/train_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "val_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/val_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/test_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# Sets the vocabulary size and sequence length\n",
    "VOCAB = 10000\n",
    "MAX_TOKENS=15\n",
    "\n",
    "# Sets the vectorization\n",
    "VECT = \"binary\"\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1417/1417 [==============================] - 6s 4ms/step - loss: 1.2833 - accuracy: 0.4331 - precision: 0.6143 - recall: 0.0543 - val_loss: 1.1468 - val_accuracy: 0.5152 - val_precision: 0.7255 - val_recall: 0.1895\n",
      "Epoch 2/20\n",
      "1417/1417 [==============================] - 5s 4ms/step - loss: 1.1066 - accuracy: 0.5251 - precision: 0.7348 - recall: 0.2273 - val_loss: 1.0991 - val_accuracy: 0.5333 - val_precision: 0.6877 - val_recall: 0.2739\n",
      "Epoch 3/20\n",
      "1417/1417 [==============================] - 5s 3ms/step - loss: 1.0282 - accuracy: 0.5630 - precision: 0.7352 - recall: 0.3132 - val_loss: 1.0914 - val_accuracy: 0.5345 - val_precision: 0.6660 - val_recall: 0.3152\n",
      "Epoch 4/20\n",
      "1417/1417 [==============================] - 4s 3ms/step - loss: 0.9762 - accuracy: 0.5918 - precision: 0.7454 - recall: 0.3686 - val_loss: 1.0975 - val_accuracy: 0.5334 - val_precision: 0.6498 - val_recall: 0.3359\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 1s 2ms/step - loss: 1.0999 - accuracy: 0.5291 - precision: 0.6369 - recall: 0.3331\n"
     ]
    }
   ],
   "source": [
    "# Sets the task name\n",
    "task_name = \"binary_vectorization\"\n",
    "\n",
    "name=task_name\n",
    "\n",
    "# Defines the results filenames\n",
    "train_name = task_name + \"_\" + \"simpsons_train\"\n",
    "val_name = task_name + \"_\" + \"simpsons_val\"\n",
    "test_name = task_name + \"_\" + \"simpsons_test\"\n",
    "\n",
    "# Creates the results file\n",
    "create_results_file(train_name)\n",
    "create_results_file(val_name)\n",
    "create_results_file(test_name)\n",
    "\n",
    "# Maps to create train text dataset\n",
    "train_text = train_ds.map(lambda x, y: x[\"sentence\"])\n",
    "\n",
    "# Creates the vectorization layer\n",
    "bin_vectorize_layer = layers.experimental.preprocessing.TextVectorization(\n",
    "    output_mode=VECT,\n",
    "    max_tokens=VOCAB,\n",
    "    pad_to_max_tokens=MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Trains the vectorization layer\n",
    "bin_vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Function to map to vectorization layer\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text[\"sentence\"], -1)\n",
    "    return bin_vectorize_layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((VOCAB, )),\n",
    "    layers.Dense(150),\n",
    "    layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", metrics.Precision(name=\"precision\"), metrics.Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[lr_decrease, early_stopping]\n",
    ")\n",
    "\n",
    "# Creates the training results\n",
    "train_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"accuracy\"][-1],\n",
    "    history.history[\"precision\"][-1],\n",
    "    history.history[\"recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the training results\n",
    "write_results_on_file(train_name, train_results)\n",
    "\n",
    "# Creates the validation results\n",
    "val_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"val_accuracy\"][-1],\n",
    "    history.history[\"val_precision\"][-1],\n",
    "    history.history[\"val_recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the validation results\n",
    "write_results_on_file(val_name, val_results)\n",
    "\n",
    "# Evaluates the model\n",
    "results = model.evaluate(\n",
    "    test_ds\n",
    ")\n",
    "\n",
    "# Appends model to list\n",
    "results.insert(0, str(model_index))\n",
    "results.insert(0, name)\n",
    "\n",
    "# Removes loss from results\n",
    "results.pop(2)\n",
    "\n",
    "# Adds F1 score to test\n",
    "results.append(calculate_f1_score(results[3], results[4]))\n",
    "\n",
    "# Writes results\n",
    "write_results_on_file(test_name, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/train_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "val_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/val_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/simpsons/test_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# Sets the vocabulary size and sequence length\n",
    "VOCAB = 10000\n",
    "MAX_TOKENS=15\n",
    "\n",
    "# Sets the vectorization\n",
    "VECT = \"tf-idf\"\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1417/1417 [==============================] - 10s 7ms/step - loss: 1.2501 - accuracy: 0.4413 - precision: 0.6229 - recall: 0.1254 - val_loss: 1.1177 - val_accuracy: 0.5254 - val_precision: 0.6738 - val_recall: 0.2838\n",
      "Epoch 2/20\n",
      "1417/1417 [==============================] - 9s 6ms/step - loss: 1.0073 - accuracy: 0.5753 - precision: 0.7309 - recall: 0.3439 - val_loss: 1.1365 - val_accuracy: 0.5218 - val_precision: 0.6325 - val_recall: 0.3412\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "304/304 [==============================] - 2s 5ms/step - loss: 1.1351 - accuracy: 0.5136 - precision: 0.6143 - recall: 0.3315\n"
     ]
    }
   ],
   "source": [
    "# Sets the task name\n",
    "task_name = \"tfidf_vectorization\"\n",
    "\n",
    "name=task_name\n",
    "\n",
    "# Defines the results filenames\n",
    "train_name = task_name + \"_\" + \"simpsons_train\"\n",
    "val_name = task_name + \"_\" + \"simpsons_val\"\n",
    "test_name = task_name + \"_\" + \"simpsons_test\"\n",
    "\n",
    "# Creates the results file\n",
    "create_results_file(train_name)\n",
    "create_results_file(val_name)\n",
    "create_results_file(test_name)\n",
    "\n",
    "# Maps to create train text dataset\n",
    "train_text = train_ds.map(lambda x, y: x[\"sentence\"])\n",
    "\n",
    "# Creates the vectorization layer\n",
    "bin_vectorize_layer = layers.experimental.preprocessing.TextVectorization(\n",
    "    output_mode=VECT,\n",
    "    max_tokens=VOCAB,\n",
    "    pad_to_max_tokens=MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Trains the vectorization layer\n",
    "bin_vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Function to map to vectorization layer\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text[\"sentence\"], -1)\n",
    "    return bin_vectorize_layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, 4)))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((VOCAB, )),\n",
    "    layers.Dense(150),\n",
    "    layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", metrics.Precision(name=\"precision\"), metrics.Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[lr_decrease, early_stopping]\n",
    ")\n",
    "\n",
    "# Creates the training results\n",
    "train_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"accuracy\"][-1],\n",
    "    history.history[\"precision\"][-1],\n",
    "    history.history[\"recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the training results\n",
    "write_results_on_file(train_name, train_results)\n",
    "\n",
    "# Creates the validation results\n",
    "val_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"val_accuracy\"][-1],\n",
    "    history.history[\"val_precision\"][-1],\n",
    "    history.history[\"val_recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the validation results\n",
    "write_results_on_file(val_name, val_results)\n",
    "\n",
    "# Evaluates the model\n",
    "results = model.evaluate(\n",
    "    test_ds\n",
    ")\n",
    "\n",
    "# Appends model to list\n",
    "results.insert(0, str(model_index))\n",
    "results.insert(0, name)\n",
    "\n",
    "# Removes loss from results\n",
    "results.pop(2)\n",
    "\n",
    "# Adds F1 score to test\n",
    "results.append(calculate_f1_score(results[3], results[4]))\n",
    "\n",
    "# Writes results\n",
    "write_results_on_file(test_name, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/train_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "val_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/val_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/test_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# Sets the vocabulary size and sequence length\n",
    "VOCAB = 15000\n",
    "MAX_TOKENS=20\n",
    "\n",
    "# Sets the vectorization\n",
    "VECT = \"binary\"\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1017/1017 [==============================] - 5s 4ms/step - loss: 1.7781 - accuracy: 0.2142 - precision: 0.0760 - recall: 4.8171e-06 - val_loss: 1.7259 - val_accuracy: 0.2670 - val_precision: 0.9091 - val_recall: 0.0014\n",
      "Epoch 2/20\n",
      "1017/1017 [==============================] - 4s 4ms/step - loss: 1.6646 - accuracy: 0.3266 - precision: 0.8042 - recall: 0.0082 - val_loss: 1.6796 - val_accuracy: 0.2934 - val_precision: 0.5929 - val_recall: 0.0311\n",
      "Epoch 3/20\n",
      "1017/1017 [==============================] - 3s 3ms/step - loss: 1.5304 - accuracy: 0.3947 - precision: 0.7875 - recall: 0.0716 - val_loss: 1.6801 - val_accuracy: 0.2997 - val_precision: 0.5274 - val_recall: 0.0677\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.6759 - accuracy: 0.3024 - precision: 0.5610 - recall: 0.0673\n"
     ]
    }
   ],
   "source": [
    "# Sets the task name\n",
    "task_name = \"binary_vectorization\"\n",
    "\n",
    "name=task_name\n",
    "\n",
    "# Defines the results filenames\n",
    "train_name = task_name + \"_\" + \"friends_train\"\n",
    "val_name = task_name + \"_\" + \"friends_val\"\n",
    "test_name = task_name + \"_\" + \"friends_test\"\n",
    "\n",
    "# Creates the results file\n",
    "create_results_file(train_name)\n",
    "create_results_file(val_name)\n",
    "create_results_file(test_name)\n",
    "\n",
    "# Maps to create train text dataset\n",
    "train_text = train_ds.map(lambda x, y: x[\"sentence\"])\n",
    "\n",
    "# Creates the vectorization layer\n",
    "bin_vectorize_layer = layers.experimental.preprocessing.TextVectorization(\n",
    "    output_mode=VECT,\n",
    "    max_tokens=VOCAB,\n",
    "    pad_to_max_tokens=MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Trains the vectorization layer\n",
    "bin_vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Function to map to vectorization layer\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text[\"sentence\"], -1)\n",
    "    return bin_vectorize_layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((VOCAB, )),\n",
    "    layers.Dense(75),\n",
    "    layers.Dense(150),\n",
    "    layers.Dense(6, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", metrics.Precision(name=\"precision\"), metrics.Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[lr_decrease, early_stopping]\n",
    ")\n",
    "\n",
    "# Creates the training results\n",
    "train_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"accuracy\"][-1],\n",
    "    history.history[\"precision\"][-1],\n",
    "    history.history[\"recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the training results\n",
    "write_results_on_file(train_name, train_results)\n",
    "\n",
    "# Creates the validation results\n",
    "val_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"val_accuracy\"][-1],\n",
    "    history.history[\"val_precision\"][-1],\n",
    "    history.history[\"val_recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the validation results\n",
    "write_results_on_file(val_name, val_results)\n",
    "\n",
    "# Evaluates the model\n",
    "results = model.evaluate(\n",
    "    test_ds\n",
    ")\n",
    "\n",
    "# Appends model to list\n",
    "results.insert(0, str(model_index))\n",
    "results.insert(0, name)\n",
    "\n",
    "# Removes loss from results\n",
    "results.pop(2)\n",
    "\n",
    "# Adds F1 score to test\n",
    "results.append(calculate_f1_score(results[3], results[4]))\n",
    "\n",
    "# Writes results\n",
    "write_results_on_file(test_name, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/train_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "val_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/val_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"./data/friends/test_ds.csv\",\n",
    "    batch_size=32,\n",
    "    column_names = [\"sentence\", \"character\"],\n",
    "    label_name=\"character\",\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# Sets the vocabulary size and sequence length\n",
    "VOCAB = 15000\n",
    "MAX_TOKENS=20\n",
    "\n",
    "# Sets the vectorization\n",
    "VECT = \"tf-idf\"\n",
    "\n",
    "# Creates the LR decrease callback\n",
    "lr_decrease = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    cooldown=3,\n",
    "    factor=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Creates a callback for early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1017/1017 [==============================] - 11s 10ms/step - loss: 1.7724 - accuracy: 0.2156 - precision: 0.4503 - recall: 0.0015 - val_loss: 1.6924 - val_accuracy: 0.2794 - val_precision: 0.6624 - val_recall: 0.0149\n",
      "Epoch 2/20\n",
      "1017/1017 [==============================] - 11s 10ms/step - loss: 1.5527 - accuracy: 0.3860 - precision: 0.8031 - recall: 0.0590 - val_loss: 1.7001 - val_accuracy: 0.2936 - val_precision: 0.5009 - val_recall: 0.0792\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "218/218 [==============================] - 2s 7ms/step - loss: 1.6953 - accuracy: 0.2985 - precision: 0.5310 - recall: 0.0775\n"
     ]
    }
   ],
   "source": [
    "# Sets the task name\n",
    "task_name = \"tfidf_vectorization\"\n",
    "\n",
    "name=task_name\n",
    "\n",
    "# Defines the results filenames\n",
    "train_name = task_name + \"_\" + \"friends_train\"\n",
    "val_name = task_name + \"_\" + \"friends_val\"\n",
    "test_name = task_name + \"_\" + \"friends_test\"\n",
    "\n",
    "# Creates the results file\n",
    "create_results_file(train_name)\n",
    "create_results_file(val_name)\n",
    "create_results_file(test_name)\n",
    "\n",
    "# Maps to create train text dataset\n",
    "train_text = train_ds.map(lambda x, y: x[\"sentence\"])\n",
    "\n",
    "# Creates the vectorization layer\n",
    "bin_vectorize_layer = layers.experimental.preprocessing.TextVectorization(\n",
    "    output_mode=VECT,\n",
    "    max_tokens=VOCAB,\n",
    "    pad_to_max_tokens=MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Trains the vectorization layer\n",
    "bin_vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Function to map to vectorization layer\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text[\"sentence\"], -1)\n",
    "    return bin_vectorize_layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, 6)))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((VOCAB, )),\n",
    "    layers.Dense(75),\n",
    "    layers.Dense(150),\n",
    "    layers.Dense(6, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", metrics.Precision(name=\"precision\"), metrics.Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[lr_decrease, early_stopping]\n",
    ")\n",
    "\n",
    "# Creates the training results\n",
    "train_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"accuracy\"][-1],\n",
    "    history.history[\"precision\"][-1],\n",
    "    history.history[\"recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"precision\"][-1], history.history[\"recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the training results\n",
    "write_results_on_file(train_name, train_results)\n",
    "\n",
    "# Creates the validation results\n",
    "val_results = [\n",
    "    name,\n",
    "    str(model_index),\n",
    "    history.history[\"val_accuracy\"][-1],\n",
    "    history.history[\"val_precision\"][-1],\n",
    "    history.history[\"val_recall\"][-1],\n",
    "    calculate_f1_score(history.history[\"val_precision\"][-1], history.history[\"val_recall\"][-1])\n",
    "]\n",
    "\n",
    "# Stores the validation results\n",
    "write_results_on_file(val_name, val_results)\n",
    "\n",
    "# Evaluates the model\n",
    "results = model.evaluate(\n",
    "    test_ds\n",
    ")\n",
    "\n",
    "# Appends model to list\n",
    "results.insert(0, str(model_index))\n",
    "results.insert(0, name)\n",
    "\n",
    "# Removes loss from results\n",
    "results.pop(2)\n",
    "\n",
    "# Adds F1 score to test\n",
    "results.append(calculate_f1_score(results[3], results[4]))\n",
    "\n",
    "# Writes results\n",
    "write_results_on_file(test_name, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
