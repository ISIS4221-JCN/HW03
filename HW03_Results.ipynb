{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW03 Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "try:\n",
    "    from gensim import models\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install gensim==3.8.0\n",
    "    from gensim import models\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install pandas\n",
    "    import pandas as pd    \n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install matplitlib\n",
    "    import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to manage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_path, data_prefix, seq_len, embedding_size):\n",
    "    \"\"\"Function to read specified data and organize it in the desired way\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): path to embedding model\n",
    "        data_prefix (str): path to data prefix\n",
    "        seq_len (int): length of each training observation\n",
    "        embedding_size (int): size of the embedding\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(path):\n",
    "        output = []\n",
    "        \n",
    "        with open(path, 'r', encoding='latin-1') as data:\n",
    "        #with open(path, 'r', encoding='utf-8') as data:\n",
    "            for row in csv.reader(data):\n",
    "                output.append(row)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    # Opens embedding model\n",
    "    model_ = models.Word2Vec.load(model_path)\n",
    "    \n",
    "    # Open dataset\n",
    "    data_train = load_data(data_prefix + \"X_train.csv\")\n",
    "    data_val = load_data(data_prefix + \"X_val.csv\")\n",
    "    data_test = load_data(data_prefix + \"X_test.csv\")\n",
    "    label_train = np.loadtxt(data_prefix + \"y_train.csv\")\n",
    "    label_val = np.loadtxt(data_prefix + \"y_val.csv\")\n",
    "    label_test = np.loadtxt(data_prefix + \"y_test.csv\")\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dt = []\n",
    "    lt = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_train):\n",
    "        \n",
    "        try:        \n",
    "            embedding = model_.wv[seq]\n",
    "            dt.append(embedding)\n",
    "            lt.append(label_train[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dv = []\n",
    "    lv = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_val):\n",
    "        \n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dv.append(embedding)\n",
    "            lv.append(label_val[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "    # Gets embeddings from model\n",
    "    dtest = []\n",
    "    ltest = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_test):\n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dtest.append(embedding)\n",
    "            ltest.append(label_test[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "    \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Pads sequences\n",
    "    dt = pad_sequences(dt, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dv = pad_sequences(dv, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dtest = pad_sequences(dtest, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    \n",
    "    # Converts lists to numpy arrays\n",
    "#     dt = np.asarray(dt).reshape((len(dt), seq_len * embedding_size))\n",
    "#     dv = np.asarray(dv).reshape((len(dv), seq_len * embedding_size))\n",
    "    \n",
    "    lt = np.asarray(lt)\n",
    "    lv = np.asarray(lv)\n",
    "    ltest = np.asarray(ltest)\n",
    "    \n",
    "    return dt, dv, lt, lv, dtest, ltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create RNN model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.enresourcesgine.sequential.Sequential: RNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.SimpleRNN(units = embedding_size, input_shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_LSTM_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create LSTM model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: LSTM model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(units = embedding_size, input_shape = input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[keras.metrics.Accuracy(), keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "# Dictionary to store model creators\n",
    "model_creators = {'SimpleRNN': create_RNN_model,\n",
    "                  'LSTM': create_LSTM_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(dataset, em_size, seq_len, model_type, epochs):\n",
    "    \"\"\"\n",
    "    Function to train the given model\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to use\n",
    "        em_size (int): size of the embedding to load\n",
    "        seq_len (int): lenght of each observation\n",
    "        model_type (str): model to train\n",
    "        epochs (int): total epochs to train the model for\n",
    "    \n",
    "    Returns:\n",
    "        PENDING.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "\n",
    "    dt, dv, lt, lv, dtest, ltest = prepare_data(data_path[em_size], data_path['prefix'], seq_len, em_size)\n",
    "    \n",
    "    \n",
    "    data = {'X_train': np.concatenate([dt,dv],axis=0), 'X_test': dtest, \n",
    "            'y_train': np.concatenate([lt,lv], axis=0), 'y_test':ltest}\n",
    "   \n",
    "    input_shape = (seq_len, em_size)\n",
    "    \n",
    "    model = model_creators[model_type](em_size, seq_len, data_path['classes'], \n",
    "                                       input_shape=input_shape)\n",
    "    history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'],data['y_test']), \n",
    "                        epochs = epochs, verbose = 0, \n",
    "                        class_weight=data_path['weights'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(dataset, embedding):\n",
    "    \"\"\" Load and retrain model to run and save results of each stage\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to deal with\n",
    "        embedding (str): Embeddings custom or inside the network\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Prediction for train\n",
    "        numpy.ndarray: True values for train\n",
    "        numpy.ndarray: Prediction for val\n",
    "        numpy.ndarray: True values for val\n",
    "        numpy.ndarray: Prediction for test\n",
    "        numpy.ndarray: True values for test\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "    \n",
    "    if embedding not in ['deepModels', 'embeddingModels']:\n",
    "        raise 'Not valid embedding type'\n",
    "        \n",
    "    results_path = './results/' + dataset + '/' + embedding + '.csv'\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    best_model = results_df[results_df.val_f1 == results_df.val_f1.max()]\n",
    "    \n",
    "    model_name = best_model['model_name'][best_model['model_name'].index[0]]\n",
    "    seq_len = best_model['seq_len'][best_model['seq_len'].index[0]]\n",
    "    em_size = best_model['embedding_size'][best_model['embedding_size'].index[0]]\n",
    "    \n",
    "    model_retrained = train_val_model(dataset, em_size, seq_len, model_name, 20)\n",
    "    \n",
    "    path = './results/' + dataset + '/checkpoints/' + str(model_name) + '_' + str(em_size) + '_' + str(seq_len)\n",
    "    model_loaded = tf.keras.models.load_model(path)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, X_test, y_test = prepare_data(data_path[em_size], \n",
    "                                                                  data_path['prefix'], \n",
    "                                                                  seq_len, \n",
    "                                                                  em_size)\n",
    "    \n",
    "    y_pred_train = model_loaded.predict(X_train)\n",
    "    y_pred_val = model_loaded.predict(X_val)\n",
    "    y_pred_test = model_retrained(X_test)\n",
    "    \n",
    "    return y_pred_train, y_train, y_pred_val, y_val, y_pred_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrices(results, dataset, embedding):\n",
    "    \"\"\" Show and save confusion matrices for given dataset and result file\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results for each stage (train, val and test)\n",
    "        dataset (str): Dataset to deal with\n",
    "        embedding (str): Embeddings custom or inside the network\n",
    "    \"\"\"\n",
    "    tasks = ['Train', 'Validation', 'Test']\n",
    "    for task in tasks:\n",
    "        plt.figure(figsize=(12,8))\n",
    "        true = results[task]['true']\n",
    "        pred = results[task]['pred']\n",
    "        sns.heatmap(confusion_matrix(np.argmax(true, axis=1), \n",
    "                                     np.argmax(pred, axis=1), normalize='pred'), cmap='Blues')\n",
    "        \n",
    "        plt.title(task + ' confusion matrix', fontsize=20)\n",
    "        plt.xlabel('Predicted', fontsize=15)\n",
    "        plt.ylabel('True', fontsize=15)\n",
    "        plt.savefig('./results/' + dataset + '/' + embedding + '/' + task + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_table(results, dataset, embedding):\n",
    "    \"\"\" Show and export to latex metrics per class in each stage\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results for each stage (train, val and test)\n",
    "        dataset (str): Dataset to deal with\n",
    "        embedding (str): Embeddings custom or inside the network\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "    \n",
    "    tasks = ['Train', 'Validation', 'Test']\n",
    "    for task in tasks:\n",
    "        true = results[task]['true']\n",
    "        pred = results[task]['pred']\n",
    "        \n",
    "        precision, recall, fscore, support = score(np.argmax(true, axis=1), np.argmax(pred, axis=1))\n",
    "        \n",
    "        df = pd.DataFrame(data={'Character': data_path['labels'], 'Precision': precision, 'Recall': recall, \n",
    "                                'F1-score': fscore, 'Support': support})\n",
    "        df.set_index('Character')\n",
    "        \n",
    "        caption = dataset.capitalize() + ' ' + embedding + ' ' + task + ' metrics per class.' \n",
    "        label = dataset + '_' + task + '_' + embedding\n",
    "        \n",
    "        text = df.to_latex(header=True, float_format=\"%.2f\", bold_rows=True, caption=caption, label=label)\n",
    "        \n",
    "        text_file = open('./results/' + dataset + '/' + embedding + '/' + task + '.txt', \"w\")\n",
    "        n = text_file.write(text)\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson_dict = {15: \"./resources/embeddings/Simpsons_15_7.model\",\n",
    "                75: \"./resources/embeddings/Simpsons_75_7.model\",\n",
    "                150: \"./resources/embeddings/Simpsons_150_7.model\",\n",
    "                'prefix': \"./data/simpsons/\",\n",
    "                'classes': 4,\n",
    "                'weights':{0:2.5, 1:1, 2:3, 3:2.5},\n",
    "                'labels': ['Lisa', 'Bart', 'Homer', 'Marge']}\n",
    "\n",
    "friends_dict = {15: \"./resources/embeddings/Friends_15_7.model\",\n",
    "                75: \"./resources/embeddings/Friends_75_7.model\",\n",
    "                150: \"./resources/embeddings/Friends_150_7.model\",\n",
    "                'prefix': \"./data/friends/\",\n",
    "                'classes': 6,\n",
    "                'weights':{0: 1, 1:1, 2:1, 3:1, 4:1, 5:1},\n",
    "                'labels': ['Monica', 'Joey', 'Chandler', 'Phoebe', 'Ross', 'Rachel']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract results for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_train, y_pred_val, y_val, y_pred_test, y_test = get_model_results('simpson', 'deepModels')\n",
    "results = {'Train': {'pred': y_pred_train, 'true': y_train},\n",
    "           'Validation': {'pred': y_pred_val, 'true': y_val},\n",
    "           'Test': {'pred': y_pred_test, 'true': y_test}}\n",
    "get_confusion_matrices(results, 'simpson', 'deepModels')\n",
    "create_result_table(results, 'simpson', 'deepModels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
