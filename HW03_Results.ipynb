{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from gensim import models\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install gensim==3.8.0\n",
    "    from gensim import models\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install matplitlib\n",
    "    import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to lead data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_path, data_prefix, seq_len, embedding_size):\n",
    "    \"\"\"Function to read specified data and organize it in the desired way\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): path to embedding model\n",
    "        data_prefix (str): path to data prefix\n",
    "        seq_len (int): length of each training observation\n",
    "        embedding_size (int): size of the embedding\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(path):\n",
    "        output = []\n",
    "        \n",
    "        with open(path, 'r', encoding='latin-1') as data:\n",
    "        #with open(path, 'r', encoding='utf-8') as data:\n",
    "            for row in csv.reader(data):\n",
    "                output.append(row)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    # Opens embedding model\n",
    "    model_ = models.Word2Vec.load(model_path)\n",
    "    \n",
    "    # Open dataset\n",
    "    data_train = load_data(data_prefix + \"X_train.csv\")\n",
    "    data_val = load_data(data_prefix + \"X_val.csv\")\n",
    "    data_test = load_data(data_prefix + \"X_test.csv\")\n",
    "    label_train = np.loadtxt(data_prefix + \"y_train.csv\")\n",
    "    label_val = np.loadtxt(data_prefix + \"y_val.csv\")\n",
    "    label_test = np.loadtxt(data_prefix + \"y_test.csv\")\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dt = []\n",
    "    lt = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_train):\n",
    "        \n",
    "        try:        \n",
    "            embedding = model_.wv[seq]\n",
    "            dt.append(embedding)\n",
    "            lt.append(label_train[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dv = []\n",
    "    lv = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_val):\n",
    "        \n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dv.append(embedding)\n",
    "            lv.append(label_val[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "    # Gets embeddings from model\n",
    "    dtest = []\n",
    "    ltest = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_test):\n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dtest.append(embedding)\n",
    "            ltest.append(label_test[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "    \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Pads sequences\n",
    "    dt = pad_sequences(dt, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dv = pad_sequences(dv, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dtest = pad_sequences(dtest, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    \n",
    "    # Converts lists to numpy arrays\n",
    "#     dt = np.asarray(dt).reshape((len(dt), seq_len * embedding_size))\n",
    "#     dv = np.asarray(dv).reshape((len(dv), seq_len * embedding_size))\n",
    "    \n",
    "    lt = np.asarray(lt)\n",
    "    lv = np.asarray(lv)\n",
    "    ltest = np.asarray(ltest)\n",
    "    \n",
    "    return dt, dv, lt, lv, dtest, ltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create RNN model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.enresourcesgine.sequential.Sequential: RNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.SimpleRNN(units = embedding_size, input_shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_LSTM_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create LSTM model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: LSTM model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(units = embedding_size, input_shape = input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[keras.metrics.Accuracy(), keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "# Dictionary to store model creators\n",
    "model_creators = {'SimpleRNN': create_RNN_model,\n",
    "                  'LSTM': create_LSTM_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson_dict = {15: \"./resources/embeddings/Simpsons_15_7.model\",\n",
    "                75: \"./resources/embeddings/Simpsons_75_7.model\",\n",
    "                150: \"./resources/embeddings/Simpsons_150_7.model\",\n",
    "                'prefix': \"./data/simpsons/\",\n",
    "                'classes': 4,\n",
    "               'weights':{0:2.5, 1:1, 2:3, 3:2.5}}\n",
    "\n",
    "friends_dict = {15: \"./resources/embeddings/Friends_15_7.model\",\n",
    "                75: \"./resources/embeddings/Friends_75_7.model\",\n",
    "                150: \"./resources/embeddings/Friends_150_7.model\",\n",
    "                'prefix': \"./data/friends/\",\n",
    "                'classes': 6,\n",
    "               'weights':{0: 1, 1:1, 2:1, 3:1, 4:1, 5:1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(dataset, em_size, seq_len, model_type, epochs):\n",
    "    \"\"\"\n",
    "    Function to train the given model\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to use\n",
    "        em_size (int): size of the embedding to load\n",
    "        seq_len (int): lenght of each observation\n",
    "        model_type (str): model to train\n",
    "        epochs (int): total epochs to train the model for\n",
    "    \n",
    "    Returns:\n",
    "        PENDING.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "\n",
    "    print(em_size)\n",
    "    dt, dv, lt, lv, dtest, ltest = prepare_data(data_path[em_size], data_path['prefix'], seq_len, em_size)\n",
    "    \n",
    "    data = {'X_train': dt+dv, 'X_test': dtest, 'y_train': lt+lv, 'y_test':ltest}\n",
    "    \n",
    "    \n",
    "    input_shape = (seq_len, em_size)\n",
    "    \n",
    "    model = model_creators[model_type](em_size, seq_len, data_path['classes'], \n",
    "                                       input_shape=input_shape)\n",
    "    history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_train'],data['y_val']), \n",
    "                        epochs = epochs, verbose = 0, \n",
    "                        class_weight=data_path['weights'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(dataset, embedding):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "    \n",
    "    if embedding not in ['deepModels', 'embeddingModels']:\n",
    "        raise 'Not valid embedding type'\n",
    "        \n",
    "    results_path = './results/' + dataset + '/' + embedding + '.csv'\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    best_model = results_df[results_df.val_f1 == results_df.val_f1.max()]\n",
    "    \n",
    "    model_retrained = train_val_model(dataset, \n",
    "                                      best_model['embedding_size'][best_model['embedding_size'].index[0]],\n",
    "                                      best_model['seq_len'][best_model['seq_len'].index[0]], \n",
    "                                      best_model['model_name'][best_model['model_name'].index[0]], 20)\n",
    "    \n",
    "    path = './results/' + dataset + '/checkpoints/'\n",
    "    path = path + best_model['model_name'] + '_' + str(best_model['embedding_size']) + '_' \n",
    "    path = path + str(best_model['seq_len'])\n",
    "    print(path)\n",
    "    model_loaded = tf.keras.models.load_model(path)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, X_test, y_test = prepare_data(data_path[em_size], \n",
    "                                                                  data_path['prefix'], \n",
    "                                                                  best_model['seq_len'], \n",
    "                                                                  best_model['embedding_size'])\n",
    "    \n",
    "    y_pred_train = model_loaded.predict(X_train)\n",
    "    y_pred_val = model_loaded.predict(X_val)\n",
    "    y_pred_test = model_retrained(X_test)\n",
    "    \n",
    "    return y_pred_train, y_train, y_pred_val, y_val, y_pred_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (26396,35,150) (5462,35,150) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e0c5ddb1619b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'simpson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deepModels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-377828b58847>\u001b[0m in \u001b[0;36mget_model_results\u001b[0;34m(dataset, embedding)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                       \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                       \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                       best_model['model_name'][best_model['model_name'].index[0]], 20)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./results/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/checkpoints/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-9c9b379525a4>\u001b[0m in \u001b[0;36mtrain_val_model\u001b[0;34m(dataset, em_size, seq_len, model_type, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mltest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mem_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mltest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (26396,35,150) (5462,35,150) "
     ]
    }
   ],
   "source": [
    "y_pred_train, y_train, y_pred_val, y_val, y_pred_test, y_test = get_model_results('simpson', 'deepModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepModels_simpson = pd.read_csv('./results/simpson/deepModels.csv')\n",
    "deepModels_friends = pd.read_csv('./results/friends/deepModels.csv')\n",
    "\n",
    "embeddings_simpson = pd.read_csv('./results/simpson/embeddingModels.csv')\n",
    "embeddings_friends = pd.read_csv('./results/friends/embeddingModels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenar modelos con embedding adentro usando datos de entrenamiento y validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, dv, lt, lv, dtest, ltest = prepare_data('./resources/embeddings/Simpsons_'+str(best_deep_simpson['embedding_size'])+'_7.model',\n",
    "                                            './data/simpsons/', \n",
    "                                            best_deep_simpson['seq_len'],\n",
    "                                            best_deep_simpson['embedding_size'])\n",
    "simpson_dict = {'X_train': dt, 'X_val': dv, 'X_test': dtest,\n",
    "               'y_train': lt, 'y_val': lv, 'y_test':ltest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepModels_simpson = pd.read_csv('./results/simpson/deepModels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_deep_simpson = deepModels_simpson[deepModels_simpson.val_accuracy == deepModels_simpson.val_accuracy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model_name</th>\n",
       "      <th>embedding_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>150</td>\n",
       "      <td>35</td>\n",
       "      <td>0.606265</td>\n",
       "      <td>0.608808</td>\n",
       "      <td>0.606265</td>\n",
       "      <td>0.60099</td>\n",
       "      <td>0.4396</td>\n",
       "      <td>0.440665</td>\n",
       "      <td>0.4396</td>\n",
       "      <td>0.434091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 model_name  embedding_size  seq_len  train_accuracy  \\\n",
       "23           0       LSTM             150       35        0.606265   \n",
       "\n",
       "    train_precision  train_recall  train_f1  val_accuracy  val_precision  \\\n",
       "23         0.608808      0.606265   0.60099        0.4396       0.440665   \n",
       "\n",
       "    val_recall    val_f1  \n",
       "23      0.4396  0.434091  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_deep_simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_deep_simpson['embedding_size'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 35, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 35, 150), dtype=tf.float32, name='lstm_11_input'), name='lstm_11_input', description=\"created by layer 'lstm_11_input'\"), but it was called on an input with incompatible shape (None, 15, 150).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = np.argmax(model.predict(simpson_dict['X_test']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix(np.argmax(simpson_dict['y_test'], axis=1), y_pred, normalize='pred'), cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31624674, 0.35013032, 0.19808862, 0.13553432],\n",
       "       [0.19140791, 0.53934496, 0.12973203, 0.1395151 ],\n",
       "       [0.31529412, 0.30823529, 0.18117647, 0.19529412],\n",
       "       [0.15641953, 0.40596745, 0.07775769, 0.35985533]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(simpson_dict['y_test'], axis=1), y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
