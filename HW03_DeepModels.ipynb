{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: silence_tensorflow in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "!pip install silence_tensorflow\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "\n",
    "try:\n",
    "    from gensim import models\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install gensim==3.8.0\n",
    "    from gensim import models\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install matplitlib\n",
    "    import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "File exists\n",
      "File exists\n",
      "File exists\n"
     ]
    }
   ],
   "source": [
    "# Create necessar folders\n",
    "try:\n",
    "    os.mkdir('./results/friends')\n",
    "except FileExistsError as e:\n",
    "    print(\"File exists\")\n",
    "try:\n",
    "    os.mkdir('./results/simpson')\n",
    "except FileExistsError as e:\n",
    "    print(\"File exists\")\n",
    "try:\n",
    "    os.mkdir('./results/friends/deepModels/')\n",
    "except FileExistsError as e:\n",
    "    print(\"File exists\")\n",
    "try:\n",
    "    os.mkdir('./results/simpson/deepModels/')\n",
    "except FileExistsError as e:\n",
    "    print(\"File exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line prevents TF crashing when using convolutional networks\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_path, data_prefix, seq_len, embedding_size):\n",
    "    \"\"\"Function to read specified data and organize it in the desired way\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): path to embedding model\n",
    "        data_prefix (str): path to data prefix\n",
    "        seq_len (int): length of each training observation\n",
    "        embedding_size (int): size of the embedding\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(path):\n",
    "        output = []\n",
    "        \n",
    "        with open(path, 'r', encoding='latin-1') as data:\n",
    "        #with open(path, 'r', encoding='utf-8') as data:\n",
    "            for row in csv.reader(data):\n",
    "                output.append(row)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    # Opens embedding model\n",
    "    model_ = models.Word2Vec.load(model_path)\n",
    "    \n",
    "    # Open dataset\n",
    "    data_train = load_data(data_prefix + \"X_train.csv\")\n",
    "    data_val = load_data(data_prefix + \"X_val.csv\")\n",
    "    label_train = np.loadtxt(data_prefix + \"y_train.csv\")\n",
    "    label_val = np.loadtxt(data_prefix + \"y_val.csv\")\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dt = []\n",
    "    lt = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_train):\n",
    "        \n",
    "        try:        \n",
    "            embedding = model_.wv[seq]\n",
    "            dt.append(embedding)\n",
    "            lt.append(label_train[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dv = []\n",
    "    lv = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_val):\n",
    "        \n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dv.append(embedding)\n",
    "            lv.append(label_val[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "    \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Pads sequences\n",
    "    dt = pad_sequences(dt, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dv = pad_sequences(dv, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    \n",
    "    # Converts lists to numpy arrays\n",
    "#     dt = np.asarray(dt).reshape((len(dt), seq_len * embedding_size))\n",
    "#     dv = np.asarray(dv).reshape((len(dv), seq_len * embedding_size))\n",
    "    \n",
    "    lt = np.asarray(lt)\n",
    "    lv = np.asarray(lv)\n",
    "    \n",
    "    return dt, dv, lt, lv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables and creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson_dict = {15: \"./resources/embeddings/Simpsons_15_7.model\",\n",
    "                75: \"./resources/embeddings/Simpsons_75_7.model\",\n",
    "                150: \"./resources/embeddings/Simpsons_150_7.model\",\n",
    "                'prefix': \"./data/simpsons/\",\n",
    "                'classes': 4,\n",
    "               'weights':{0:2.5, 1:1, 2:3, 3:2.5}}\n",
    "\n",
    "friends_dict = {15: \"./resources/embeddings/Friends_15_7.model\",\n",
    "                75: \"./resources/embeddings/Friends_75_7.model\",\n",
    "                150: \"./resources/embeddings/Friends_150_7.model\",\n",
    "                'prefix': \"./data/friends/\",\n",
    "                'classes': 6,\n",
    "               'weights':{0: 1, 1:1, 2:1, 3:1, 4:1, 5:1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_callback(filepath):\n",
    "    \"\"\"\n",
    "    Function to create instance of keras early stop callback\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to save the model\n",
    "    \"\"\"\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath,\n",
    "        save_weights_only = False,\n",
    "        monitor = 'val_accuracy',\n",
    "        mode = 'max',\n",
    "        save_best_only = True)\n",
    "    return model_checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_early_stop_callback():\n",
    "    \"\"\"\n",
    "    Function to create instance of keras early stop callback\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0,\n",
    "    mode = 'min', baseline = None, restore_best_weights = False)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create baseline model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: base model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\", input_shape = input_shape),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deepFC_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create deepFC model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): lengt'baseline','DeepFC', 'SimpleRNN', h of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: deep fully conected model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\", input_shape = input_shape),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense((embedding_size * seq_len)/4, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense((embedding_size * seq_len)/16, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create RNN model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: RNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.SimpleRNN(units = embedding_size, input_shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create LSTM model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: LSTM model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(units = embedding_size, input_shape = input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/8, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[keras.metrics.Accuracy(), keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model creators\n",
    "model_creators = {'baseline': create_base_model,\n",
    "                  'DeepFC': create_deepFC_model,\n",
    "                  'SimpleRNN': create_RNN_model,\n",
    "                  'LSTM': create_LSTM_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(dataset, em_size, seq_len, model_type, epochs):\n",
    "    \"\"\"\n",
    "    Function to train the given model\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to use\n",
    "        em_size (int): size of the embedding to load\n",
    "        seq_len (int): lenght of each observation\n",
    "        model_type (str): model to train\n",
    "        epochs (int): total epochs to train the model for\n",
    "    \n",
    "    Returns:\n",
    "        PENDING.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "        \n",
    "    X_train, X_val, y_train, y_val = prepare_data(data_path[em_size], data_path['prefix'],\n",
    "                                                  seq_len, em_size)\n",
    "    \n",
    "    if model_type in  ['baseline', 'DeepFC']:\n",
    "        X_train = np.asarray(X_train).reshape((len(X_train), seq_len * em_size))\n",
    "        X_val = np.asarray(X_val).reshape((len(X_val), seq_len * em_size))\n",
    "    \n",
    "    checkpoint_callback = create_checkpoint_callback('./results/' + dataset + '/deepModels/checkpoints/' + model_type + '_' + \n",
    "                                          str(em_size) + '_' +str(seq_len))\n",
    "    \n",
    "    early_stop_callback = create_early_stop_callback()\n",
    "    \n",
    "    input_shape = (len(X_train), seq_len * em_size) if model_type in ['baseline','DeepFC'] else (seq_len, em_size)\n",
    "    \n",
    "    model = model_creators[model_type](em_size, seq_len, data_path['classes'], \n",
    "                                       input_shape=input_shape)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs = epochs, verbose = 0, \n",
    "                        callbacks=[checkpoint_callback, early_stop_callback], \n",
    "                        class_weight=data_path['weights'])\n",
    "    \n",
    "    y_pred_train = np.argmax(model.predict(X_train), axis = 1)\n",
    "    y_pred_val = np.argmax(model.predict(X_val), axis = 1)\n",
    "    \n",
    "    y_train = np.argmax(y_train, axis = 1)\n",
    "    y_val = np.argmax(y_val, axis = 1)\n",
    "    \n",
    "    sample_weights_train = [data_path['weights'][i] for i in y_train]\n",
    "    sample_weights_val = [data_path['weights'][i] for i in y_val]\n",
    "    \n",
    "    data = {'model_name': [model_type], 'embedding_size': [em_size], 'seq_len': [seq_len],\n",
    "            'train_accuracy': [accuracy_score(y_train, y_pred_train, sample_weight=sample_weights_train)],\n",
    "            'train_precision': [precision_score(y_train, y_pred_train, sample_weight=sample_weights_train, average='weighted',zero_division=0)],\n",
    "            'train_recall': [recall_score(y_train, y_pred_train, sample_weight=sample_weights_train, average='weighted',zero_division=0)], \n",
    "            'train_f1': [f1_score(y_train, y_pred_train, sample_weight=sample_weights_train, average='weighted',zero_division=0)], \n",
    "            'val_accuracy': [accuracy_score(y_val, y_pred_val, sample_weight=sample_weights_val)], \n",
    "            'val_precision': [precision_score(y_val, y_pred_val, sample_weight=sample_weights_val, average='weighted',zero_division=0)], \n",
    "            'val_recall': [recall_score(y_val, y_pred_val, sample_weight=sample_weights_val, average='weighted',zero_division=0)], \n",
    "            'val_f1': [f1_score(y_val, y_pred_val, sample_weight=sample_weights_val, average='weighted',zero_division=0)]}\n",
    "    \n",
    "    return pd.DataFrame(data=data), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN_ = [15, 35, 50]\n",
    "EM_SIZE_ = [15, 75, 150]\n",
    "MODEL_TYPES_ = ['baseline','DeepFC', 'SimpleRNN', 'LSTM']\n",
    "metrics_df = pd.DataFrame()\n",
    "histories = []\n",
    "for seq_len in SEQ_LEN_:\n",
    "    for em_size in EM_SIZE_:\n",
    "        for model_type in MODEL_TYPES_:\n",
    "            model_results, history = train_val_model('simpson', em_size, seq_len, model_type, 20)\n",
    "            metrics_df = pd.concat([metrics_df, model_results])\n",
    "            histories.append(history)\n",
    "metrics_df.to_csv('./results/simpson/deepModels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN_ = [15, 35, 50]\n",
    "EM_SIZE_ = [15, 75, 150]\n",
    "MODEL_TYPES_ = ['baseline','DeepFC', 'SimpleRNN', 'LSTM']\n",
    "metrics_df = pd.DataFrame()\n",
    "for seq_len in SEQ_LEN_:\n",
    "    for em_size in EM_SIZE_:\n",
    "        for model_type in MODEL_TYPES_:\n",
    "            model_results, history = train_val_model('friends', em_size, seq_len, model_type, 20)\n",
    "            metrics_df = pd.concat([metrics_df, model_results])\n",
    "            histories.append(history)\n",
    "metrics_df.to_csv('./results/friends/deepModels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
