{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from gensim import models\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install gensim==3.8.0\n",
    "    from gensim import models\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError as e:\n",
    "    !pip install matplitlib\n",
    "    import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line prevents TF crashing when using convolutional networks\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_path, data_prefix, seq_len, embedding_size):\n",
    "    \"\"\"Function to read specified data and organize it in the desired way\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): path to embedding model\n",
    "        data_prefix (str): path to data prefix\n",
    "        seq_len (int): length of each training observation\n",
    "        embedding_size (int): size of the embedding\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(path):\n",
    "        output = []\n",
    "        \n",
    "        with open(path, 'r', encoding='latin-1') as data:\n",
    "        #with open(path, 'r', encoding='utf-8') as data:\n",
    "            for row in csv.reader(data):\n",
    "                output.append(row)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    # Opens embedding model\n",
    "    model_ = models.Word2Vec.load(model_path)\n",
    "    \n",
    "    # Open dataset\n",
    "    data_train = load_data(data_prefix + \"X_train.csv\")\n",
    "    data_val = load_data(data_prefix + \"X_val.csv\")\n",
    "    label_train = np.loadtxt(data_prefix + \"y_train.csv\")\n",
    "    label_val = np.loadtxt(data_prefix + \"y_val.csv\")\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dt = []\n",
    "    lt = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_train):\n",
    "        \n",
    "        try:        \n",
    "            embedding = model_.wv[seq]\n",
    "            dt.append(embedding)\n",
    "            lt.append(label_train[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "            \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Gets embeddings from model\n",
    "    dv = []\n",
    "    lv = []\n",
    "    omissions_ = 0\n",
    "    \n",
    "    for i, seq in enumerate(data_val):\n",
    "        \n",
    "        try:\n",
    "            embedding = model_.wv[seq]\n",
    "            dv.append(embedding)\n",
    "            lv.append(label_val[i])\n",
    "        \n",
    "        except KeyError as ke:\n",
    "            for word in seq:\n",
    "                if word not in model_.wv.vocab.keys():\n",
    "                    seq.remove(word)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            omissions_ += 1\n",
    "    \n",
    "#     print(omissions_)\n",
    "    \n",
    "    # Pads sequences\n",
    "    dt = pad_sequences(dt, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    dv = pad_sequences(dv, padding='post', dtype='float64', maxlen=seq_len)\n",
    "    \n",
    "    # Converts lists to numpy arrays\n",
    "#     dt = np.asarray(dt).reshape((len(dt), seq_len * embedding_size))\n",
    "#     dv = np.asarray(dv).reshape((len(dv), seq_len * embedding_size))\n",
    "    \n",
    "    lt = np.asarray(lt)\n",
    "    lv = np.asarray(lv)\n",
    "    \n",
    "    return dt, dv, lt, lv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables and creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson_dict = {15: \"./resources/embeddings/Simpsons_15_7.model\",\n",
    "                75: \"./resources/embeddings/Simpsons_75_7.model\",\n",
    "                150: \"./resources/embeddings/Simpsons_150_7.model\",\n",
    "                'prefix': \"./data/simpsons/\",\n",
    "                'classes': 4}\n",
    "\n",
    "friends_dict = {5: \"./resources/embeddings/Friends_5_7.model\",\n",
    "                25: \"./resources/embeddings/Friends_25_7.model\",\n",
    "                125: \"./resources/embDictionaries to organize codeeddings/Friends_125_7.model\",\n",
    "                'prefix': \"./data/friends/\",\n",
    "                'classes': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_callback(filepath):\n",
    "    \"\"\"\n",
    "    Function to create instance of keras early stop callback\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to save the model\n",
    "    \"\"\"\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath,\n",
    "        save_weights_only = False,\n",
    "        monitor = 'val_loss',\n",
    "        mode = 'min',\n",
    "        save_best_only = True)\n",
    "    return model_checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_early_stop_callback():\n",
    "    \"\"\"\n",
    "    Function to create instance of keras early stop callback\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0, patience = 30, verbose = 0,\n",
    "    mode = 'min', baseline = None, restore_best_weights = False)\n",
    "    return callbacklength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create baseline model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: base model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\", input_shape = input_shape),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deepFC_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create deepFC model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: deep fully conected model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\", input_shape = input_shape),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create RNN model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: RNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.SimpleRNN(units = 75, input_shape=input_shape),\n",
    "#         layers.SimpleRNN(units = 20, return_sequences=True),\n",
    "#         layers.SimpleRNN(units = 20, return_sequences=True),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(embedding_size, seq_len, classes, input_shape):\n",
    "    \"\"\"\n",
    "    Function to create LSTM model\n",
    "    \n",
    "    Args:\n",
    "        embedding_size (int): Size of the embedding to be used\n",
    "        seq_len (int): length of each training observation\n",
    "        classes (int): number of possible classes\n",
    "        input_shape (int): Shape in which the input will be provided\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.python.keras.engine.sequential.Sequential: LSTM model \n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(units = 75, input_shape = input_shape),\n",
    "#         layers.LSTM(units = 20, return_sequences=True),\n",
    "#         layers.LSTM(units = 20, return_sequences=True),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense((embedding_size * seq_len)/2, activation=\"relu\"),\n",
    "        layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model creators\n",
    "model_creators = {'baseline': create_base_model,\n",
    "                  'DeepFC': create_deepFC_model,\n",
    "                  'SimpleRNN': create_RNN_model,\n",
    "                  'LSTM': create_LSTM_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(dataset, em_size, seq_len, model_type, epochs):\n",
    "    \"\"\"\n",
    "    Function to train the given model\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to use\n",
    "        em_size (int): size of the embedding to load\n",
    "        seq_len (int): lenght of each observation\n",
    "        model_type (str): model to train\n",
    "        epochs (int): total epochs to train the model for\n",
    "    \n",
    "    Returns:\n",
    "        PENDING.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'simpson':\n",
    "        data_path = simpson_dict\n",
    "    elif dataset == 'friends':\n",
    "        data_path = friends_dict\n",
    "    else: \n",
    "        raise 'Not valid dataset'\n",
    "        \n",
    "    X_train, X_val, y_train, y_val = prepare_data(data_path[em_size], data_path['prefix'],\n",
    "                                                  seq_len, em_size)\n",
    "    \n",
    "    if model_type in  ['baseline', 'DeepFC']:\n",
    "        X_train = np.asarray(X_train).reshape((len(X_train), seq_len * em_size))\n",
    "        X_val = np.asarray(X_val).reshape((len(X_val), seq_len * em_size))\n",
    "    \n",
    "    checkpoint_callback = create_checkpoint_callback('./resources/checkpoints/' + model_type + '_' + \n",
    "                                          str(em_size) + '_' +str(seq_len))\n",
    "    \n",
    "    early_stop_callback = create_early_stop_callback()\n",
    "    \n",
    "    input_shape = (len(X_train), seq_len * em_size) if model_type in ['baseline','DeepFC'] else (seq_len, em_size)\n",
    "    \n",
    "    model = model_creators[model_type](em_size, seq_len, data_path['classes'], \n",
    "                                       input_shape=input_shape)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "              epochs = epochs, verbose = 0, \n",
    "              callbacks=[checkpoint_callback, early_stop_callback])\n",
    "    \n",
    "    model_metrics_train = model.evaluate(x = X_train, y = y_train)\n",
    "    model_metrics_val = model.evaluate(x = X_val, y = y_val)\n",
    "    data = {'model_name': [model_type], 'embedding_size': [em_size], 'seq_len': [seq_len],\n",
    "            'train_accuracy': [model_metrics_train[1]], 'train_precision': [model_metrics_train[2]], 'train_recall': [model_metrics_train[3]],\n",
    "           'val_accuracy': [model_metrics_val[1]], 'val_precision': [model_metrics_val[2]], 'val_recall': [model_metrics_val[3]]}\n",
    "    \n",
    "    return pd.DataFrame(data=data), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN_ = [8, 15, 25, 30, 50]\n",
    "EM_SIZE_ = [15,75,150]\n",
    "MODEL_TYPES_ = ['baseline','DeepFC', 'SimpleRNN', 'LSTM']\n",
    "metrics_df = pd.DataFrame()\n",
    "histories = []\n",
    "for seq_len in SEQ_LEN_:\n",
    "    for em_size in EM_SIZE_:\n",
    "        for model_type in MODEL_TYPES_:\n",
    "            model_results, history = train_val_model('simpson', em_size, seq_len, model_type, 1000)\n",
    "            metrics_df = pd.concat([metrics_df, model_results])\n",
    "            histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN_ = [8, 15, 25, 30, 50]\n",
    "EM_SIZE_ = [5, 25, 125]\n",
    "MODEL_TYPES_ = ['baseline','DeepFC', 'SimpleRNN', 'LSTM']\n",
    "metrics_df1 = pd.DataFrame()\n",
    "for seq_len in SEQ_LEN_:\n",
    "    for em_size in EM_SIZE_:\n",
    "        for model_type in MODEL_TYPES_:0.2077\n",
    "            model_results, history = train_val_model('friends', em_size, seq_len, model_type, 1000)\n",
    "            metrics_df1 = pd.concat([metrics_df1, model_results])\n",
    "            histories.append(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
